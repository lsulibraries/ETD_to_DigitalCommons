{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "\n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for scrutinizing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_all_sheets(workbook):\n",
    "    # non-essential sanity check function\n",
    "    sheets = [sheet for sheet in available_wb.get_sheet_names()]\n",
    "    print(sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_all_info(urn):\n",
    "    # non-essential script that prints all the info related to a urn.\n",
    "    print(\"Main example:\", main_sheet[urn], \"\\n\")\n",
    "    print(\"Filenames example:\", filenames_sheet[urn], \"\\n\")\n",
    "    print(\"Keywords example:\", keywords_sheet[urn], \"\\n\")\n",
    "    print(\"Advisors example:\", advisors_sheet[urn], \"\\n\")\n",
    "    print(\"Catalog example:\", catalog_sheet[urn], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_combinations_of_advisors(advisors_sheet):\n",
    "    urn_advisortitles = dict()\n",
    "    for urn, advisors_nt_list in advisors_sheet.items():\n",
    "        for item in advisors_nt_list:\n",
    "            if item.urn in urn_advisortitles:\n",
    "                urn_advisortitles[item.urn].append(item.advisor_title)\n",
    "            else:\n",
    "                urn_advisortitles[item.urn] = [item.advisor_title, ]\n",
    "\n",
    "    a_set = set()\n",
    "    for urn, titles in urn_advisortitles.items():\n",
    "        for title in titles:\n",
    "            a_set.add(title)\n",
    "    print(a_set)\n",
    "\n",
    "    advisors_permutations = set()\n",
    "\n",
    "    for urn, titles in urn_advisortitles.items():\n",
    "        this_permutation = (titles.count('Committee Chair'),\n",
    "                            titles.count('Committee Co-Chair'),\n",
    "                            titles.count('Committee Member'),\n",
    "                            titles.count(\"Dean's Representative\"),\n",
    "                            )\n",
    "        advisors_permutations.add(this_permutation)\n",
    "    for i in advisors_permutations:\n",
    "        print(i)\n",
    "    return advisors_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_mismatching_files(filenames_sheet):\n",
    "    sames = dict()\n",
    "    for urn, filenames_namedtuple_list in filenames_sheet.items():\n",
    "        for item in filenames_namedtuple_list:\n",
    "            if item.urn in sames:\n",
    "                if sames[item.urn] != item.availability:\n",
    "                    print('there should be one {}'.format(item.urn))\n",
    "            else:\n",
    "                sames[item.urn] = item.availability\n",
    "    return sames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_misnamed_extensions(filenames_sheet):\n",
    "    misnamed_urn_filename = []\n",
    "    for urn, filenames_namedtuple_list in filenames_sheet.items():\n",
    "        for item in filenames_namedtuple_list:\n",
    "            if item.filename[-4] != \".\" and item.filename[-4:] not in (\"docx\", \"r.gz\"):\n",
    "                misnamed_urn_filename.append((urn, item.filename))\n",
    "                print(urn, item.filename)\n",
    "    return misnamed_urn_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_legacy_school_names(main_sheet):\n",
    "    schools_etds = dict()\n",
    "    for urn, itemnamedtuple in main_sheet.items():\n",
    "        if itemnamedtuple.department in schools_etds:\n",
    "            schools_etds[itemnamedtuple.department].append(urn)\n",
    "        else:\n",
    "            schools_etds[itemnamedtuple.department] = [urn, ]\n",
    "    for school, urns in schools_etds.items():\n",
    "        print(school)\n",
    "    return schools_etds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_page_by_page_pdfs(filenames_sheet):\n",
    "    split_files = dict()\n",
    "    for urn, filenames_namedtuples_list in filenames_sheet.items():\n",
    "        for item in filenames_namedtuples_list:\n",
    "            if item.urn not in split_files:\n",
    "                split_files[item.urn] = [item.filename, ]\n",
    "            else:\n",
    "                split_files[item.urn].append(item.filename)\n",
    "    page_by_page_pdfs = []\n",
    "    for urn, filelist in split_files.items():\n",
    "        split = False\n",
    "        for i in filelist:\n",
    "            if \"chap\" in i.lower():\n",
    "                split = True\n",
    "        if len(filelist) > 1 and split == True:\n",
    "            print(urn, '\\n', filelist, '\\n')\n",
    "            page_by_page_pdfs.append((urn, filelist))\n",
    "    return page_by_page_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_catalog_superset_of_database(catalog_sheet, main_sheet):\n",
    "    outside_uris = []\n",
    "    for uri in catalog_sheet:\n",
    "        if uri not in main_sheet:\n",
    "            # print(uri)\n",
    "            outside_uris.append(uri)\n",
    "    print(len(outside_uris))\n",
    "    return outside_uris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading & parsing source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_workbook(workbook_name):\n",
    "    sourcepath = 'data/databasetables'\n",
    "    filename = 'prod_etd_{}_database.xlsx'.format(workbook_name)\n",
    "    fullpath = os.path.join(sourcepath, filename)\n",
    "    return openpyxl.load_workbook(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_main_sheet(all_db_workbooks):\n",
    "    \"\"\" returns a dictionary in form of:\n",
    "    {urn: NamedTuple\n",
    "     urn: NamedTuple\n",
    "    }\n",
    "    NamedTuple is expected to have attributes: (urn first_name middle_name last_name suffix author_email\n",
    "                                                publish_email degree department dtype title abstract availability\n",
    "                                                availability_description copyright_statement ddate sdate adate\n",
    "                                                cdate rdate pid url notice notice_response timestamp\n",
    "                                                survey_completed)\n",
    "                                            or: (urn first_name middle_name last_name suffix author_email\n",
    "                                                publish_email degree department dtype title abstract availability\n",
    "                                                availability_description copyright_statement ddate sdate adate\n",
    "                                                cdate rdate pid url notices timestamp)\n",
    "    \"\"\"\n",
    "    (available_wb, submitted_wb, withheld_wb) = all_db_workbooks\n",
    "    main_dict = dict()\n",
    "    for wb in (available_wb, submitted_wb, withheld_wb):\n",
    "        current_sheet = wb.get_sheet_by_name('etd_main table')\n",
    "        for num, row in enumerate(current_sheet.iter_rows()):\n",
    "            if num == 0:\n",
    "                headers = (i.value for i in row)\n",
    "                MainSheet = namedtuple('MainSheet', headers)\n",
    "                continue\n",
    "            values = (i.value for i in row)\n",
    "            item = MainSheet(*values)\n",
    "            main_dict[item.urn] = item\n",
    "    return main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_filename_sheet(all_db_workbooks):\n",
    "    \"\"\" returns a dictionary in form of:\n",
    "        urn: [NamedTuple, NamedTuple, ],\n",
    "        urn: [NamedTuple, ]\n",
    "    NamedTuple is expected to have attributes (path, size, available, description, page_count, timestamp)\n",
    "    \"\"\"\n",
    "    (available_wb, submitted_wb, withheld_wb) = all_db_workbooks\n",
    "    filenames_sheet = dict()\n",
    "    for wb in (available_wb, submitted_wb, withheld_wb):\n",
    "        current_sheet = wb.get_sheet_by_name('filename_by_urn table')\n",
    "        for num, row in enumerate(current_sheet.iter_rows()):\n",
    "            if num == 0:\n",
    "                headers = (i.value for i in row)\n",
    "                Filenames = namedtuple('Filenames', headers)\n",
    "                continue\n",
    "            values = (i.value for i in row)\n",
    "            item = Filenames(*values)\n",
    "\n",
    "            if item.urn not in filenames_sheet:\n",
    "                filenames_sheet[item.urn] = [item, ]\n",
    "            else:\n",
    "                row_timestamp = datetime.strptime(item.timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "                if item.filename in filenames_sheet[item.urn]:\n",
    "                    previous_filename_entry = [i for i in filenames_sheet[item.urn] if i.filename == item.filename]\n",
    "                    previous_timestamp = datetime.strptime(previous_filename_entry[0].timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    if row_timestamp > previous_timestamp:\n",
    "                        print('oops')\n",
    "                        previous_filename_entry[0] = item\n",
    "                else:\n",
    "                    filenames_sheet[item.urn].append(item)\n",
    "    return filenames_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_keyword_sheet(all_db_workbooks):\n",
    "    \"\"\" returns a dictionary in form of:\n",
    "    {urn: [NamedTuple,\n",
    "           NamedTuple,\n",
    "           ]}\n",
    "    NamedTuple is expected to have attributes ('keyword', 'urn', 'timestamp')\n",
    "    \"\"\"\n",
    "    (available_wb, submitted_wb, withheld_wb) = all_db_workbooks\n",
    "    keywords_sheet = dict()\n",
    "    for wb in (available_wb, submitted_wb, withheld_wb):\n",
    "        current_sheet = wb.get_sheet_by_name('keyword_by_urn table')\n",
    "        for num, row in enumerate(current_sheet.iter_rows()):\n",
    "            if num == 0:\n",
    "                headers = (i.value for i in row)\n",
    "                Keywords = namedtuple('Keywords', headers)\n",
    "                continue\n",
    "            values = (i.value for i in row)\n",
    "            item = Keywords(*values)\n",
    "            if item.urn not in keywords_sheet:\n",
    "                keywords_sheet[item.urn] = [item, ]\n",
    "            else:\n",
    "                keywords_sheet[item.urn].append(item)\n",
    "    return keywords_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_advisors_sheet(all_db_workbooks):\n",
    "    \"\"\" returns a dictionary in form of:\n",
    "        {urn: [NamedTuple,\n",
    "               NamedTuple,\n",
    "               ]}\n",
    "        NamedTuple is expected to have attributes ('urn', 'advisor_name', 'advisor_title',\n",
    "                                                   'advisor_email', 'approval', 'timestamp')\n",
    "   \"\"\"\n",
    "    advisors_sheet = dict()\n",
    "    (available_wb, submitted_wb, withheld_wb) = all_db_workbooks\n",
    "    for wb in (available_wb, submitted_wb, withheld_wb):\n",
    "        current_sheet = wb.get_sheet_by_name('advisor_by_urn table')\n",
    "        for num, row in enumerate(current_sheet.iter_rows()):\n",
    "            if num == 0:\n",
    "                headers = (i.value for i in row)\n",
    "                Advisors = namedtuple('Advisor', headers)\n",
    "                continue\n",
    "            values = (i.value for i in row)\n",
    "            item = Advisors(*values)\n",
    "            if item.urn not in advisors_sheet:\n",
    "                advisors_sheet[item.urn] = [item, ]\n",
    "            else:\n",
    "                row_timestamp = datetime.strptime(item.timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "                if item.advisor_name in advisors_sheet[item.urn]:\n",
    "                    previous_advisor_entry = [i for i in advisors_sheet[item.urn] if i.advisor_name == item.advisor_name]\n",
    "                    previous_timestamp = datetime.strptime(previous_advisor_entry[0].timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    if row_timestamp > previous_timestamp:\n",
    "                        previous_advisor_entry[0] = item\n",
    "                else:\n",
    "                    advisors_sheet[item.urn].append(item)\n",
    "    return advisors_sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_catalog_sheet():\n",
    "    \"\"\" returns a dictionary in form of:\n",
    "        {urn: NamedTuple\n",
    "         urn: NamedTuple}\n",
    "    \"\"\"\n",
    "    catalog_sheet = dict()\n",
    "    sourcepath = 'data/Catalogtables'\n",
    "    sourcefile = 'CatalogETDSelectMetadata.csv'\n",
    "    with open(os.path.join(sourcepath, sourcefile), encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for num, row in enumerate(csvreader):\n",
    "            if num == 0:\n",
    "                headers = (i for i in row)\n",
    "                Catalog = namedtuple('Catalog', headers)\n",
    "                continue\n",
    "            values = (i for i in row)\n",
    "            item = Catalog(*values)\n",
    "            urn = [i for i in os.path.split(item.URL) if 'etd-' in i]\n",
    "            urn = os.path.split(urn[0])[1]\n",
    "            if not urn:\n",
    "                print('No urn for URL:', item.URL)\n",
    "            else:\n",
    "                catalog_sheet[urn] = item\n",
    "    return catalog_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_binary(url):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        return response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_binary_to_file(binary, folder, filename):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, 'bw') as f:\n",
    "        f.write(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_postupload = ('etd-06182004-122626', 'etd-09012004-114224', 'etd-0327102-091522', 'etd-0707103-142120',\n",
    "                     'etd-0710102-054039', 'etd-0409103-184148', 'etd-04152004-142117', 'etd-0830102-145811',\n",
    "                     'etd-0903103-141852', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_binaries(filenames_sheet):\n",
    "    didnt_grab = []\n",
    "    target_dir = './ETDbinaries/'\n",
    "    count = 0\n",
    "    for urn, filenames_namedtuples_list in filenames_sheet.items():\n",
    "        local_dir = os.path.join(target_dir, urn)\n",
    "        local_files = []\n",
    "        if os.path.isdir(local_dir):\n",
    "            local_files = os.listdir(local_dir)\n",
    "        for item in filenames_namedtuples_list:\n",
    "            if item.filename in local_files:\n",
    "                pass\n",
    "            else:\n",
    "                url = 'http://etd.lsu.edu/{}/{}'.format(\"/\".join(item.path.split('/')[3:]),\n",
    "                                                                 item.filename)\n",
    "                try:\n",
    "                    binary = retrieve_binary(url)\n",
    "                    write_binary_to_file(binary, local_dir, item.filename)\n",
    "                except:\n",
    "                    count += 1\n",
    "                    pass\n",
    "                    didnt_grab.append((urn, item.filename))\n",
    "                    print(urn, item.filename)\n",
    "    print(count)\n",
    "    return didnt_grab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_writer(data, path):\n",
    "    with open(path, \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "        for line in data:\n",
    "            writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatinate_keywords(keywords_sheet, urn):\n",
    "    if urn in keywords_sheet:\n",
    "        return ', '.join(nt.keyword for nt in keywords_sheet[urn] if nt.keyword)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_advisors(advisors_sheet, urn):\n",
    "    Advisor = namedtuple('Advisor', ('urn', 'advisor_name', 'advisor_title',\n",
    "                                           'advisor_email', 'approval', 'timestamp'))\n",
    "    blank_Advisor = Advisor('', '', '', '', '', '')\n",
    "    Advisors_nt = advisors_sheet[urn]\n",
    "    advisors_rank = {'Committee Chair': 1, 'Committee Co-Chair': 2, 'Committee Member': 3, \"Dean's Representative\": 4}\n",
    "    alpha_Advisors = sorted(Advisors_nt, key=lambda x: x.advisor_name)\n",
    "    sorted_advisors = sorted(alpha_Advisors, key=lambda x: advisors_rank[x.advisor_title])\n",
    "    if len(sorted_advisors) > 7 and \"Dean's Representative\" in sorted_advisors[-1]:\n",
    "        sorted_advisors = sorted_advisors[:6] + sorted_advisors[-1:]\n",
    "    elif len(sorted_advisors) > 7:\n",
    "        sorted_advisors = sorted_advisors[:7]\n",
    "    elif len(sorted_advisors) < 7:\n",
    "        missing = 7 - len(sorted_advisors)\n",
    "        for i in range(missing):\n",
    "            sorted_advisors.append(blank_Advisor)\n",
    "    return sorted_advisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_slash_and_padding(text):\n",
    "    if not text:\n",
    "        return\n",
    "    text = text.strip()\n",
    "    if text[-1] == '/':\n",
    "        text = text[:-1]\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_null_with_nothing(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    return text.replace('NULL', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combine_title(catalog_sheet, main_sheet, urn):\n",
    "    if urn in catalog_sheet:\n",
    "        title = catalog_sheet[urn].Title\n",
    "        title = strip_slash_and_padding(title)\n",
    "        subtitle = catalog_sheet[urn].Subtitle\n",
    "        subtitle = strip_slash_and_padding(subtitle)\n",
    "        if title[-1] == ':':\n",
    "            title = title[:-1]\n",
    "        if subtitle:\n",
    "            csv_title = \"{}:  {}\".format(title,\n",
    "                                         subtitle)\n",
    "        else:\n",
    "            csv_title = title\n",
    "    else:\n",
    "        title = main_sheet[urn].title\n",
    "        title = strip_slash_and_padding(title)\n",
    "        csv_title = title\n",
    "    return csv_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_all_brackets(text):\n",
    "    for i in ('<', '>', '[', ']', '{', '}', '(', ')'):\n",
    "        text = text.replace(i, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_pub_date(catalog_sheet, urn):\n",
    "    if urn in catalog_sheet:\n",
    "        if catalog_sheet[urn].SeriesDate:\n",
    "            return remove_all_brackets(catalog_sheet[urn].SeriesDate)\n",
    "        if catalog_sheet[urn].PubDate:\n",
    "            return remove_all_brackets(catalog_sheet[urn].PubDate)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_defense_date(main_sheet, urn):\n",
    "    if main_sheet[urn].ddate:\n",
    "        return remove_all_brackets(main_sheet[urn].ddate)[:4]\n",
    "    return ''\n",
    "\n",
    "def find_submission_date(main_sheet, urn):\n",
    "    if main_sheet[urn].sdate:\n",
    "        return remove_all_brackets(main_sheet[urn].sdate)[:4]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_csv(main_sheet, catalog_sheet, filenames_sheet, keywords_sheet, advisors_sheet):\n",
    "    csv_data = []\n",
    "\n",
    "    csvfieldnames = [\"title\",\n",
    "                     \"urn\",\n",
    "                     \"author_fname\",\n",
    "                     'author_mname',\n",
    "                     'author_lname',\n",
    "                     'author_suffix',\n",
    "                     'author Email',\n",
    "                     'author_email_pub',\n",
    "                     'advisor1_title',\n",
    "                     'advisor1_name',\n",
    "                     \"advisor1_email\",\n",
    "                     'advisor2_title',\n",
    "                     'advisor2_name',\n",
    "                     \"advisor2_email\",\n",
    "                     'advisor3_title',\n",
    "                     'advisor3_name',\n",
    "                     \"advisor3_email\",\n",
    "                     'advisor4_title',\n",
    "                     'advisor4_name',\n",
    "                     \"advisor4_email\",\n",
    "                     'advisor5_title',\n",
    "                     'advisor5_name',\n",
    "                     \"advisor5_email\",\n",
    "                     'advisor6_title',\n",
    "                     'advisor6_name',\n",
    "                     \"advisor6_email\",\n",
    "                     'advisor7_title',\n",
    "                     'advisor7_name',\n",
    "                     \"advisor7_email\",\n",
    "                     \"document_type\",\n",
    "                     'degree_name',\n",
    "                     'department',\n",
    "                     'legacy_department',\n",
    "                     'disciplines',\n",
    "                     'keywords',\n",
    "                     'abstract',\n",
    "                     'publication_date',\n",
    "                     'defense_date',\n",
    "                     'submission_date',\n",
    "                     'availability',\n",
    "                     'availability_description',\n",
    "                    ]\n",
    "    csv_data.append(csvfieldnames)\n",
    "    for urn in main_sheet:\n",
    "        csv_title = combine_title(catalog_sheet, main_sheet, urn)\n",
    "        csv_urn = urn\n",
    "        csv_first_name = main_sheet[urn].first_name\n",
    "        csv_middle_name = main_sheet[urn].middle_name\n",
    "        csv_last_name = main_sheet[urn].last_name\n",
    "        csv_suffix = main_sheet[urn].suffix\n",
    "        csv_suffix = replace_null_with_nothing(csv_suffix)\n",
    "        csv_author_email = main_sheet[urn].author_email\n",
    "        csv_publish_email = main_sheet[urn].publish_email\n",
    "        sorted_advisors = organize_advisors(advisors_sheet, urn)\n",
    "        csv_document_type = main_sheet[urn].dtype\n",
    "        csv_degree = main_sheet[urn].degree\n",
    "        csv_legacy_department = main_sheet[urn].department\n",
    "        csv_department = 'awaiting mapping legacy:current'\n",
    "        csv_disciplines = 'awaiting mapping ???:disciplines'\n",
    "        csv_keywords = concatinate_keywords(keywords_sheet, urn)\n",
    "        csv_abstract = main_sheet[urn].abstract\n",
    "        csv_publication_date = find_pub_date(catalog_sheet, urn)\n",
    "        csv_defense_date = find_defense_date(main_sheet, urn)\n",
    "        csv_submission_date = find_submission_date(main_sheet, urn)\n",
    "        csv_availability = main_sheet[urn].availability\n",
    "        csv_availability_desc = main_sheet[urn].availability_description,\n",
    "        \n",
    "        \n",
    "        \n",
    "        if urn in filenames_sheet:\n",
    "            filename = filenames_sheet[urn][0].filename\n",
    "        else:\n",
    "            filename = ''\n",
    "            \n",
    "        csv_data.append([csv_title,\n",
    "                         csv_urn,\n",
    "                         csv_first_name,\n",
    "                         csv_middle_name,\n",
    "                         csv_last_name,\n",
    "                         csv_suffix,\n",
    "                         csv_author_email,\n",
    "                         csv_publish_email,\n",
    "                         sorted_advisors[0].advisor_title,\n",
    "                         sorted_advisors[0].advisor_name,\n",
    "                         sorted_advisors[0].advisor_email,\n",
    "                         sorted_advisors[1].advisor_title,\n",
    "                         sorted_advisors[1].advisor_name,\n",
    "                         sorted_advisors[1].advisor_email,\n",
    "                         sorted_advisors[2].advisor_title,\n",
    "                         sorted_advisors[2].advisor_name,\n",
    "                         sorted_advisors[2].advisor_email,\n",
    "                         sorted_advisors[3].advisor_title,\n",
    "                         sorted_advisors[3].advisor_name,\n",
    "                         sorted_advisors[3].advisor_email,\n",
    "                         sorted_advisors[4].advisor_title,\n",
    "                         sorted_advisors[4].advisor_name,\n",
    "                         sorted_advisors[4].advisor_email,\n",
    "                         sorted_advisors[5].advisor_title,\n",
    "                         sorted_advisors[5].advisor_name,\n",
    "                         sorted_advisors[5].advisor_email,\n",
    "                         sorted_advisors[6].advisor_title,\n",
    "                         sorted_advisors[6].advisor_name,\n",
    "                         sorted_advisors[6].advisor_email,\n",
    "                         csv_document_type,\n",
    "                         csv_degree,\n",
    "                         csv_legacy_department,\n",
    "                         csv_department,\n",
    "                         csv_disciplines,\n",
    "                         csv_keywords,\n",
    "                         csv_abstract,\n",
    "                         csv_publication_date,\n",
    "                         csv_defense_date,\n",
    "                         csv_submission_date,\n",
    "                         csv_availability,\n",
    "                         csv_availability_desc,\n",
    "                         ])\n",
    "#     print(csv_data)\n",
    "    csv_writer(csv_data, '../../trash.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "available_wb = read_workbook('available')\n",
    "submitted_wb = read_workbook('submitted')\n",
    "withheld_wb = read_workbook('withheld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merges the matching sheets from all 3 workbooks into one datastructure per sheet-type.\n",
    "all_db_workbooks = (available_wb, submitted_wb, withheld_wb)\n",
    "\n",
    "main_sheet = parse_main_sheet(all_db_workbooks)\n",
    "filenames_sheet = parse_filename_sheet(all_db_workbooks)\n",
    "keywords_sheet = parse_keyword_sheet(all_db_workbooks)\n",
    "advisors_sheet = parse_advisors_sheet(all_db_workbooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catalog_sheet = parse_catalog_sheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_csv(main_sheet, catalog_sheet, filenames_sheet, keywords_sheet, advisors_sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
